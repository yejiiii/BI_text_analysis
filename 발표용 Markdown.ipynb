{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Last week Issue\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "### Feature Engineering\n",
    "---------------------------------------------------------------------\n",
    "- n-gram\n",
    "- K-fold\n",
    "- 형용사,부사,동사\n",
    "- 긍정/부정 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\Miniconda3\\envs\\workspace\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import nltk\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open(\"C:\\Workspace\\\\Project\\\\BI_text_analysis\\\\data\\\\reviews_Books_5.json\",\"r\") as data_file:\n",
    "    raw_data=data_file.readlines()\n",
    "\n",
    "json_list = []\n",
    "for i in range(len(raw_data)):\n",
    "    try:\n",
    "        json_data = json.loads(raw_data[i])\n",
    "        json_list.append(json_data)\n",
    "    except:\n",
    "        pass\n",
    "review_data = []\n",
    "\n",
    "for i in range(len(json_list)):\n",
    "    review_dict={}\n",
    "    review_dict[\"reviewText\"] = json_list[i][\"reviewText\"]\n",
    "    review_dict[\"overall\"] = json_list[i][\"overall\"]\n",
    "    review_data.append(review_dict)\n",
    "    \n",
    "import random\n",
    "random_data = random.sample(review_data,len(review_data))\n",
    "\n",
    "reviewText = []\n",
    "for i in range(len(random_data)):\n",
    "    reviewText.append(random_data[i][\"reviewText\"].lower())\n",
    "\n",
    "y_data = []\n",
    "int_y_data = []\n",
    "for i in range(len(random_data)):\n",
    "    y_data.append(random_data[i][\"overall\"])\n",
    "y_data=np.array(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## n-gram & K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.632782310004\n",
      "Logistic Regression : 0.617539716617\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(reviewText)\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 형용사,부사,동사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "SVM : 0.632782310004\n",
      "Logistic Regression : 0.617539716617\n"
     ]
    }
   ],
   "source": [
    "def pos_tagging(document):\n",
    "    result = nltk.pos_tag( nltk.word_tokenize(document))    \n",
    "    last_result = ['/'.join(t) for t in result]\n",
    "    return last_result\n",
    "\n",
    "tag = [pos_tagging(i) for i in reviewText]\n",
    "tagging_VB_RB_JJ = []\n",
    "for i in range(len(reviewText[:1000])):\n",
    "    tagging = []\n",
    "    for j in tag[i]:\n",
    "        if j.split(\"/\")[1] == \"JJ\" or j.split(\"/\")[1] == \"JJR\" or j.split(\"/\")[1] == \"JJS\" or j.split(\"/\")[1] == \"RB\" or j.split(\"/\")[1] == \"RBR\" or j.split(\"/\")[1] == \"RBS\" or j.split(\"/\")[1] == \"VB\" or j.split(\"/\")[1] == \"VBD\" or j.split(\"/\")[1] == \"VBG\" or j.split(\"/\")[1] == \"VBN\" or j.split(\"/\")[1] == \"VBP\" or j.split(\"/\")[1] == \"VBZ\":\n",
    "            tagging.append(j.split(\"/\")[0])\n",
    "    join_each_tag = \" \".join(tagging)\n",
    "    tagging_VB_RB_JJ.append(join_each_tag)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(tagging_VB_RB_J)\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 긍정/부정 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.632782310004\n",
      "Logistic Regression : 0.617539716617\n"
     ]
    }
   ],
   "source": [
    "# readline_all.py\n",
    "neg=[]\n",
    "f = open(\"C:\\Workspace\\\\Project\\\\BI_text_analysis\\\\data\\\\neg.txt\", 'r')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    neg.append(line)\n",
    "f.close()\n",
    "\n",
    "# readline_all.py\n",
    "pos=[]\n",
    "f = open(\"C:\\Workspace\\\\Project\\\\BI_text_analysis\\\\data\\\\pos.txt\", 'r')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    pos.append(line)\n",
    "f.close()\n",
    "\n",
    "def delete_blank(document):\n",
    "    result=[]\n",
    "    for i in range(len(document)):\n",
    "        if i % 2 == 0:\n",
    "            result.append(document[i])\n",
    "    return result\n",
    "\n",
    "pos_Text = delete_blank(pos)\n",
    "neg_Text = delete_blank(neg)\n",
    "\n",
    "vailidation_Text = pos_Text+neg_Text\n",
    "\n",
    "vailidation_result = []\n",
    "for i in vailidation_Text:\n",
    "    a = i.split(\"\\n\")\n",
    "    vailidation_result.append(a[0])\n",
    "\n",
    "filtered_sentence_result = []\n",
    "join_text = []\n",
    "\n",
    "\n",
    "for i in reviewText:\n",
    "    word_tokens = word_tokenize(i)\n",
    "    filtered_sentence = [w for w in word_tokens if w in vailidation_result]\n",
    "    filtered_sentence_result.append(filtered_sentence)\n",
    "\n",
    "join_filtered_sentence_result = []\n",
    "\n",
    "for i in filtered_sentence_result:\n",
    "    a = \" \".join(i)\n",
    "    join_filtered_sentence_result.append(a)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(join_filtered_sentence_result)\n",
    "\n",
    "y_data = []\n",
    "int_y_data = []\n",
    "for i in range(len(random_data)):\n",
    "    y_data.append(random_data[i][\"overall\"])\n",
    "y_data=np.array(y_data)\n",
    "\n",
    "\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model 선택\n",
    "----------------------------------------------------------\n",
    "- SVM\n",
    "- Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# This week Issue\n",
    "----------------------------------------\n",
    "### Variables\n",
    "--------------------------------------------\n",
    "- Max_df, Min_df ratio\n",
    "- Test set ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  7 Techniques to Handle Imbalanced Data(Resample the training set)\n",
    "-----------------------------------------------------------------\n",
    "- Bagging\n",
    "- RandomForest\n",
    "- ExtraTrees\n",
    "- AdaBoost\n",
    "- GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(reviewText)\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "seed = 7\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"Bagging :\",accuracy_score(result.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"RandomForest :\",accuracy_score(result.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"ExtraTrees :\",accuracy_score(result.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "seed = 7\n",
    "num_trees = 30\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"AdaBoost :\",accuracy_score(result.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"GradientBoosting :\",accuracy_score(result.predict(X_test),y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
