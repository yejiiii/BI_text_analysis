{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/reviews_Books_5.json\",\"r\") as data_file:\n",
    "    raw_data=data_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"reviewerID\": \"AAXUNK0W2DZG5\", \"asin\": \"0060520841\", \"reviewerName\": \"Amazon Customer \\\"leneker\\\"\", \"helpful\": [5, 10], \"reviewText\": \"1996 Bernard Goldberg wrote an editorial for the Wall Street Journal that said there was an obvious bias on the part of network new shows  for the liberal point of view.  he then illustrated this with an example that he dissected in-depth.  The reaction to this observation was the ruination of her career, and the beginning of his status as a pariah among most newsmen.  This book is used to add more ammo to the controversy.That the journalists who so eagerly pry into other peoples lives and business should be reluctant to be examined is hardly surprising.  Almost no one really wants to have his life or business dissected by Mike Wallace not even Dan Rather.  Some facts in this book are really potent such as the survey results which show how the average journalist and the average American are often vastly at odds.  Other chapters highlight different stories that TV news has covered and the analysis that Goldberg has made to point out the liberal bias.  His main theme is that while there is no conspiracy of the left, the fact is that most reporters are liberal but fewer still will admit it.There is some sound reasoning behind this book.  Sadly in execution, it doesn't come off as well as it could.  Goldberg has one argument in all the years he worked closely with Dan Rather (according to him) and yet it seems like all they did was fight.  Why ?  because Goldberg replays that one argument about 5 or 6 times in the book.  Much of the material that is thought provoking the first time around is pretty stale after the third or fourth reading. He kindly reprints the editorial that started the whole furor, too bad this was at the end of the book because the entire first chapter is just a rehash of that argument.  Too often Goldberg is reduced sounding like the bitter vindictive perso 93184\n"
     ]
    }
   ],
   "source": [
    "json_list = []\n",
    "for i in range(len(raw_data)):\n",
    "    try:\n",
    "        json_data = json.loads(raw_data[i])\n",
    "        json_list.append(json_data)\n",
    "    except:\n",
    "        print(raw_data[i],i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_data = []\n",
    "for i in range(len(json_list)):\n",
    "    if (len(json_list[i][\"reviewText\"])!=0):\n",
    "        review_dict={}\n",
    "        review_dict[\"reviewText\"] = json_list[i][\"reviewText\"]\n",
    "        review_dict[\"overall\"] = json_list[i][\"overall\"]\n",
    "        review_data.append(review_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random_data = random.sample(review_data,len(review_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviewText = []\n",
    "for i in range(len(random_data)):\n",
    "    reviewText.append(random_data[i][\"reviewText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data = []\n",
    "for i in range(len(random_data)):\n",
    "    y_data.append(random_data[i][\"overall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 4239, 2.0: 5102, 3.0: 10176, 4.0: 21555, 5.0: 52105})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>데이터 전처리</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "   \n",
    "    cleaned_text_list=[]\n",
    "    for i in range(len(raw_review)):\n",
    "        \n",
    "        review_text = BeautifulSoup(raw_review[i],\"html.parser\").get_text()\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "        words = letters_only.lower().split()\n",
    "        cleaned_text = \" \".join(words)\n",
    "        cleaned_text_list.append(cleaned_text)\n",
    "\n",
    "    return cleaned_text_list\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pre_data=review_to_words(reviewText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가장 작은 평점으로 갯수 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "length=sorted(Counter(y_data).most_common())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "search=[1.0,2.0,3.0,4.0,5.0]\n",
    "count=0\n",
    "text=[]\n",
    "y=[]\n",
    "\n",
    "for i in search:\n",
    "    count=0\n",
    "    for index,value in enumerate(y_data):\n",
    "        if (count==length*3): break\n",
    "        elif (i==value):\n",
    "            text.append(pre_data[index])\n",
    "            y.append(value)\n",
    "            count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 4239, 2.0: 5102, 3.0: 10176, 4.0: 12717, 5.0: 12717})"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>데이터 셔플 다시</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = list(zip(text, y))\n",
    "random.shuffle(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, b = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewText=list(a)\n",
    "overall=np.array(list(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True,ngram_range=(1,2),lowercase=True,min_df=2,max_df=0.2,stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(tfidf , overall, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30117, 378023), (30117,), (14834, 378023), (14834,))"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "#logreg = linear_model.LogisticRegression(random_state=500,solver='sag',multi_class='multinomial',warm_start=True)\n",
    "logreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48779830119994605"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(logreg.predict(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.27372263,  0.09187279,  0.44850988,  0.52301759,  0.71310312])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(logreg.predict(x_test), y_test,average=None) \n",
    "#e scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.62      0.27      0.38      1370\n",
      "        2.0       0.40      0.09      0.15      1698\n",
      "        3.0       0.44      0.45      0.44      3389\n",
      "        4.0       0.44      0.52      0.48      4149\n",
      "        5.0       0.55      0.71      0.62      4228\n",
      "\n",
      "avg / total       0.48      0.49      0.46     14834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, logreg.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 점 =  0.272736272124\n",
      "2 점 =  0.270778928316\n",
      "3 점 =  0.345166235015\n",
      "4 점 =  0.556204329755\n",
      "5 점 =  0.785862472127\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "for i in range(1,6):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, logreg.predict(x_test), pos_label=i)\n",
    "#pos_label Label considered as positive and others are considered negative.\n",
    "    print(i,\"점 = \",metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\"\"\"I enjoyed this book, as long as I kept in mind that the book was written for young adults.  \n",
    "         The characters were believable and it was easy to identify with their pain.\"\"\",\n",
    "        \"I think the author gets lost in his own thoughts and trys to make storys of something that's not there.\",\n",
    "        \"\"\"I was very excited to start reading this book and although I did like it, \n",
    "        I thought that a lot of the words to describe body parts was kind of high-schoolish.  \n",
    "        Didn't care for the cliff hanging ending.\"\"\",\n",
    "        \"\"\"I don't know if you are familiar with FANFIC on the internet but that what this book reminded me of.  \n",
    "        I love the series and thought I would like the books...not so much.\"\"\",\n",
    "        \"\"\"As I love western era romances, this one was wonderful.  An easy read and ended the way we always hope....happily!\"\"\"]\n",
    "overall=[4,1,3,2,5]\n",
    "vecs = vectorizer.transform(texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 3, 2, 5] [ 4.  3.  3.  5.  5.]\n"
     ]
    }
   ],
   "source": [
    "print(overall,logreg.predict(vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.  3.  3.  4.  3.  3.  3.  3.  3.  3.  3.  5.  5.  3.  3.] [ 4.  4.  3.  4.  4.  4.  4.  3.  5.  5.  3.  5.  5.  3.  3.]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:15],logreg.predict(x_test)[:15])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
