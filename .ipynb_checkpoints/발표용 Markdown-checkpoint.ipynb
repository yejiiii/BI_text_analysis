{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Last week Issue\n",
    "-------------------------------------------------------------------\n",
    "\n",
    "### Feature Engineering\n",
    "---------------------------------------------------------------------\n",
    "- n-gram\n",
    "- K-fold\n",
    "- 형용사,부사,동사\n",
    "- 긍정/부정 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\Miniconda3\\envs\\workspace\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import nltk\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open(\"C:\\Workspace\\\\Project\\\\BI_text_analysis\\\\data\\\\reviews_Books_5.json\",\"r\") as data_file:\n",
    "    raw_data=data_file.readlines()\n",
    "\n",
    "json_list = []\n",
    "for i in range(len(raw_data)):\n",
    "    try:\n",
    "        json_data = json.loads(raw_data[i])\n",
    "        json_list.append(json_data)\n",
    "    except:\n",
    "        pass\n",
    "review_data = []\n",
    "\n",
    "for i in range(len(json_list)):\n",
    "    review_dict={}\n",
    "    review_dict[\"reviewText\"] = json_list[i][\"reviewText\"]\n",
    "    review_dict[\"overall\"] = json_list[i][\"overall\"]\n",
    "    review_data.append(review_dict)\n",
    "    \n",
    "import random\n",
    "random_data = random.sample(review_data,len(review_data))\n",
    "\n",
    "reviewText = []\n",
    "for i in range(len(random_data)):\n",
    "    reviewText.append(random_data[i][\"reviewText\"].lower())\n",
    "\n",
    "y_data = []\n",
    "int_y_data = []\n",
    "for i in range(len(random_data)):\n",
    "    y_data.append(random_data[i][\"overall\"])\n",
    "y_data=np.array(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## n-gram & K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.625697724345\n",
      "Logistic Regression : 0.622048089309\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), max_df = 0.2, min_df =2, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(reviewText)\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 형용사,부사,동사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.625697724345\n",
      "Logistic Regression : 0.622048089309\n"
     ]
    }
   ],
   "source": [
    "def pos_tagging(document):\n",
    "    result = nltk.pos_tag( nltk.word_tokenize(document))    \n",
    "    last_result = ['/'.join(t) for t in result]\n",
    "    return last_result\n",
    "\n",
    "tag = [pos_tagging(i) for i in reviewText]\n",
    "tagging_VB_RB_JJ = []\n",
    "for i in range(len(reviewText)):\n",
    "    tagging = []\n",
    "    for j in tag[i]:\n",
    "        if j.split(\"/\")[1] == \"JJ\" or j.split(\"/\")[1] == \"JJR\" or j.split(\"/\")[1] == \"JJS\" or j.split(\"/\")[1] == \"RB\" or j.split(\"/\")[1] == \"RBR\" or j.split(\"/\")[1] == \"RBS\" or j.split(\"/\")[1] == \"VB\" or j.split(\"/\")[1] == \"VBD\" or j.split(\"/\")[1] == \"VBG\" or j.split(\"/\")[1] == \"VBN\" or j.split(\"/\")[1] == \"VBP\" or j.split(\"/\")[1] == \"VBZ\":\n",
    "            tagging.append(j.split(\"/\")[0])\n",
    "    join_each_tag = \" \".join(tagging)\n",
    "    tagging_VB_RB_JJ.append(join_each_tag)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), max_df = 0.2, min_df =2, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(tagging_VB_RB_JJ)\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.597252039502\n",
      "Logistic Regression : 0.615392872477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), max_df = 0.1, min_df = 28, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(tagging_VB_RB_JJ)\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.602082438815\n",
      "Logistic Regression : 0.609918419923\n"
     ]
    }
   ],
   "source": [
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 긍정/부정 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.556462000859\n",
      "Logistic Regression : 0.582760841563\n"
     ]
    }
   ],
   "source": [
    "# readline_all.py\n",
    "neg=[]\n",
    "f = open(\"C:\\Workspace\\\\Project\\\\BI_text_analysis\\\\data\\\\neg.txt\", 'r')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    neg.append(line)\n",
    "f.close()\n",
    "\n",
    "# readline_all.py\n",
    "pos=[]\n",
    "f = open(\"C:\\Workspace\\\\Project\\\\BI_text_analysis\\\\data\\\\pos.txt\", 'r')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    pos.append(line)\n",
    "f.close()\n",
    "\n",
    "def delete_blank(document):\n",
    "    result=[]\n",
    "    for i in range(len(document)):\n",
    "        if i % 2 == 0:\n",
    "            result.append(document[i])\n",
    "    return result\n",
    "\n",
    "pos_Text = delete_blank(pos)\n",
    "neg_Text = delete_blank(neg)\n",
    "\n",
    "vailidation_Text = pos_Text+neg_Text\n",
    "\n",
    "vailidation_result = []\n",
    "for i in vailidation_Text:\n",
    "    a = i.split(\"\\n\")\n",
    "    vailidation_result.append(a[0])\n",
    "\n",
    "filtered_sentence_result = []\n",
    "join_text = []\n",
    "\n",
    "\n",
    "for i in reviewText:\n",
    "    word_tokens = word_tokenize(i)\n",
    "    filtered_sentence = [w for w in word_tokens if w in vailidation_result]\n",
    "    filtered_sentence_result.append(filtered_sentence)\n",
    "\n",
    "join_filtered_sentence_result = []\n",
    "\n",
    "for i in filtered_sentence_result:\n",
    "    a = \" \".join(i)\n",
    "    join_filtered_sentence_result.append(a)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), max_df = 0.2, min_df =2, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(join_filtered_sentence_result)\n",
    "\n",
    "y_data = []\n",
    "int_y_data = []\n",
    "for i in range(len(random_data)):\n",
    "    y_data.append(random_data[i][\"overall\"])\n",
    "y_data=np.array(y_data)\n",
    "\n",
    "\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.625697724345\n",
      "Logistic Regression : 0.622048089309\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), max_df = 0.2, min_df = 2, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(reviewText)\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.609596393302\n",
      "Logistic Regression : 0.585659081151\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(2,2), max_df = 0.2, min_df =2, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(reviewText)\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.610133104337\n",
      "Logistic Regression : 0.583512237012\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(2,3), max_df = 0.2, min_df =2, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(reviewText)\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.605410047231\n",
      "Logistic Regression : 0.622906826964\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(2,1), max_df = 0.2, min_df = 2, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(reviewText)\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.605410047231\n",
      "Logistic Regression : 0.622906826964\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(3,1), max_df = 0.2, min_df = 2, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(reviewText)\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-8c1293b13e7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviewText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mskf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\samsung\\Miniconda3\\envs\\workspace\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1350\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \"\"\"\n\u001b[1;32m-> 1352\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1353\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\samsung\\Miniconda3\\envs\\workspace\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 839\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\samsung\\Miniconda3\\envs\\workspace\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    782\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    783\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(3,2), max_df = 0.2, min_df =2, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(reviewText)\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM : 0.560004293688\n",
      "Logistic Regression : 0.563546586518\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(3,3), max_df = 0.2, min_df =2, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(reviewText)\n",
    "\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "   \n",
    "    cleaned_text_list=[]\n",
    "    for i in range(len(raw_review)):\n",
    "        \n",
    "        review_text = BeautifulSoup(raw_review[i]).get_text()\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "        words = letters_only.lower().split()\n",
    "        cleaned_text = \" \".join(words)\n",
    "        cleaned_text_list.append(cleaned_text)\n",
    "\n",
    "    return cleaned_text_list\n",
    "\n",
    "\n",
    "pre_data=review_to_words(reviewText)\n",
    "\n",
    "c = list(zip(pre_data, y_data))\n",
    "random.shuffle(c)\n",
    "\n",
    "a, b = zip(*c)\n",
    "\n",
    "pre_text_data=list(a)\n",
    "overall_data=np.array(list(b))\n",
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), max_df = 0.2, min_df = 2, lowercase=True, stop_words = 'english')\n",
    "\n",
    "tfidf = vectorizer.fit_transform(text_data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(tfidf , overall , test_size=0.33, random_state=100)\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsung\\Miniconda3\\envs\\workspace\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\samsung\\Miniconda3\\envs\\workspace\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0eb0add8dd0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_data' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    \n",
    "   \n",
    "    cleaned_text_list=[]\n",
    "    for i in range(len(raw_review)):\n",
    "        \n",
    "        review_text = BeautifulSoup(raw_review[i]).get_text()\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "        words = letters_only.lower().split()\n",
    "        cleaned_text = \" \".join(words)\n",
    "        cleaned_text_list.append(cleaned_text)\n",
    "\n",
    "    return cleaned_text_list\n",
    "\n",
    "\n",
    "pre_data=review_to_words(reviewText)\n",
    "\n",
    "c = list(zip(pre_data, y_data))\n",
    "random.shuffle(c)\n",
    "\n",
    "a, b = zip(*c)\n",
    "\n",
    "pre_text_data=list(a)\n",
    "overall_data=np.array(list(b))\n",
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), max_df = 0.1, min_df =28, lowercase=True, stop_words = 'english')\n",
    "\n",
    "tfidf = vectorizer.fit_transform(text_data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(tfidf , overall , test_size=0.33, random_state=100)\n",
    "\n",
    "result = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"SVM :\",accuracy_score(result.predict(X_test),y_test))\n",
    "\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg.predict(X_test) == y_test\n",
    "\n",
    "print(\"Logistic Regression :\",accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model 선택\n",
    "----------------------------------------------------------\n",
    "- SVM\n",
    "- Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# This week Issue\n",
    "----------------------------------------\n",
    "### Variables\n",
    "--------------------------------------------\n",
    "- Max_df, Min_df ratio\n",
    "- Test set ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  7 Techniques to Handle Imbalanced Data(Resample the training set)\n",
    "-----------------------------------------------------------------\n",
    "- Bagging\n",
    "- RandomForest\n",
    "- ExtraTrees\n",
    "- AdaBoost\n",
    "- GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', sublinear_tf=True, ngram_range=(1,2), max_df = 0.2, min_df =2, lowercase=True, stop_words = 'english')\n",
    "D = vectorizer.fit_transform(join_filtered_sentence_result)\n",
    "skf = cross_validation.StratifiedKFold(y_data, n_folds=10,random_state=500)\n",
    "for train_index, test_index in skf:\n",
    "    X_train, X_test = D[train_index], D[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "seed = 7\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"Bagging :\",accuracy_score(result.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"RandomForest :\",accuracy_score(result.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"ExtraTrees :\",accuracy_score(result.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "seed = 7\n",
    "num_trees = 30\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"AdaBoost :\",accuracy_score(result.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X_train, y_train)\n",
    "result.predict(X_test)\n",
    "print(\"GradientBoosting :\",accuracy_score(result.predict(X_test),y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
